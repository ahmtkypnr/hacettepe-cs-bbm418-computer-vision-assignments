{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1679,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import structural_similarity as ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1680,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hough_transform(edge_image, theta_steps=180, rho_step=1):\n",
    "    \"\"\"\n",
    "    Corrected Hough Transform for line detection using raw image coordinates.\n",
    "    \n",
    "    Parameters:\n",
    "    edge_image (numpy.ndarray): Binary edge image (0s and 1s).\n",
    "    theta_steps (int): Number of angular bins (0 to 180 degrees).\n",
    "    rho_step (float): Resolution step for rho (pixels).\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (accumulator, thetas, rhos)\n",
    "        accumulator: 2D Hough accumulator array.\n",
    "        thetas: Array of theta values (radians).\n",
    "        rhos: Array of rho values (pixels).\n",
    "    \"\"\"\n",
    "    height, width = edge_image.shape\n",
    "    diag = int(np.ceil(np.hypot(height, width)))\n",
    "    thetas = np.deg2rad(np.linspace(0, 180, theta_steps, endpoint=False))\n",
    "    \n",
    "    # Rho range covers -diag to diag, adjusted for rho_step\n",
    "    max_rho = diag\n",
    "    rhos = np.arange(-max_rho, max_rho + rho_step, rho_step)\n",
    "    accumulator = np.zeros((len(rhos), theta_steps), dtype=np.uint64)\n",
    "    \n",
    "    # Get edge coordinates (no y-axis flipping)\n",
    "    y_img, x = np.nonzero(edge_image)\n",
    "    \n",
    "    # Vectorized computation of rho for all (x, y) and thetas\n",
    "    cos_theta = np.cos(thetas)\n",
    "    sin_theta = np.sin(thetas)\n",
    "    \n",
    "    # Shape: (N_edges, theta_steps)\n",
    "    rho_values = x[:, np.newaxis] * cos_theta + y_img[:, np.newaxis] * sin_theta\n",
    "    \n",
    "    # Quantize rho to bins\n",
    "    rho_indices = np.round(\n",
    "        (rho_values + max_rho) / rho_step  # Shift to non-negative\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Filter valid indices\n",
    "    valid_mask = (rho_indices >= 0) & (rho_indices < len(rhos))\n",
    "    theta_indices = np.tile(np.arange(theta_steps), (len(x), 1))\n",
    "    \n",
    "    # Update accumulator using valid indices\n",
    "    np.add.at(\n",
    "        accumulator, \n",
    "        (rho_indices[valid_mask], theta_indices[valid_mask]), \n",
    "        1\n",
    "    )\n",
    "    \n",
    "    return accumulator, thetas, rhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1681,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hough_lines(accumulator, thetas, rhos, threshold=200, neighborhood_size=20):\n",
    "    lines = []\n",
    "    half_size = neighborhood_size // 2\n",
    "    # Get candidate indices where accumulator > threshold\n",
    "    candidate_indices = np.argwhere(accumulator > threshold)\n",
    "    \n",
    "    for rho_idx, theta_idx in candidate_indices:\n",
    "        candidate_value = accumulator[rho_idx, theta_idx]\n",
    "        \n",
    "        # Define neighborhood bounds (handling borders)\n",
    "        rho_min = max(rho_idx - half_size, 0)\n",
    "        rho_max = min(rho_idx + half_size + 1, accumulator.shape[0])\n",
    "        theta_min = max(theta_idx - half_size, 0)\n",
    "        theta_max = min(theta_idx + half_size + 1, accumulator.shape[1])\n",
    "        \n",
    "        # Extract the neighborhood\n",
    "        neighborhood = accumulator[rho_min:rho_max, theta_min:theta_max]\n",
    "        \n",
    "        # Only add the candidate if it is the local maximum in its neighborhood\n",
    "        if candidate_value == np.max(neighborhood):\n",
    "            rho = rhos[rho_idx]\n",
    "            theta = thetas[theta_idx]\n",
    "            lines.append((rho, theta))\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1682,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ransac_line_vectorized(data, k, t, d):\n",
    "    \"\"\"\n",
    "    Vectorized RANSAC algorithm to fit a line to data with outliers.\n",
    "\n",
    "    Parameters:\n",
    "    - data: array of shape (N, 2) where each row is (x, y)\n",
    "    - k: maximum number of iterations\n",
    "    - t: threshold value to determine when a data point fits the model\n",
    "    - d: minimum number of inliers required to accept a model\n",
    "\n",
    "    Returns:\n",
    "    - best_model: tuple (m, c) for the line model y = m*x + c, or None if no valid model is found\n",
    "    - best_consensus_set: array of indices of inlier points for the best model\n",
    "    \"\"\"\n",
    "    best_model = None\n",
    "    best_consensus_set = None\n",
    "    best_inlier_count = 0\n",
    "    best_error = np.inf\n",
    "\n",
    "    for _ in range(k):\n",
    "        # Randomly select n distinct points\n",
    "        sample_indices = random.sample(range(data.shape[0]), 2)\n",
    "        sample = data[sample_indices]\n",
    "\n",
    "        # Skip vertical lines to avoid division by zero\n",
    "        if sample[1, 0] == sample[0, 0]:\n",
    "            continue\n",
    "\n",
    "        # Compute line parameters: y = m*x + c\n",
    "        m = (sample[1, 1] - sample[0, 1]) / (sample[1, 0] - sample[0, 0])\n",
    "        c = sample[0, 1] - m * sample[0, 0]\n",
    "\n",
    "        # Vectorized computation of distances of all points to the line\n",
    "        distances = np.abs(m * data[:, 0] - data[:, 1] + c) / np.sqrt(m**2 + 1)\n",
    "        consensus_set = np.where(distances < t)[0]\n",
    "        inlier_count = consensus_set.size\n",
    "\n",
    "        if inlier_count >= d:\n",
    "            error = np.mean(distances[consensus_set])\n",
    "            # Update best model if more inliers are found or if equal, but with lower error\n",
    "            if inlier_count > best_inlier_count or (inlier_count == best_inlier_count and error < best_error):\n",
    "                best_inlier_count = inlier_count\n",
    "                best_error = error\n",
    "                best_model = (m, c)\n",
    "                best_consensus_set = consensus_set\n",
    "\n",
    "    return best_model, best_consensus_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1683,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_line_to_rho_theta(m, c):\n",
    "    \"\"\"\n",
    "    Converts a line from slope-intercept form y = m*x + c to the polar form\n",
    "    rho = x*cos(theta) + y*sin(theta) where rho >= 0 and theta in [0, π).\n",
    "\n",
    "    Parameters:\n",
    "    - m: slope of the line\n",
    "    - c: intercept of the line\n",
    "\n",
    "    Returns:\n",
    "    - (rho, theta): tuple with the polar coordinates\n",
    "    \"\"\"\n",
    "    # Using the line in form: m*x - y + c = 0\n",
    "    # Normal vector is (m, -1) and its norm is sqrt(m^2+1)\n",
    "    theta = np.arctan2(-1, m)\n",
    "    rho = -c / np.sqrt(m**2 + 1)\n",
    "    \n",
    "    # Ensure that rho is nonnegative\n",
    "    if rho < 0:\n",
    "        rho = -rho\n",
    "        theta += np.pi\n",
    "        \n",
    "    # Normalize theta to be in the range [0, π)\n",
    "    theta = theta % np.pi\n",
    "    return rho, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1684,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_multiple_lines(data, k, t, d, min_remaining_points=100):\n",
    "    \"\"\"\n",
    "    Iteratively applies RANSAC to find multiple lines.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: array of shape (N, 2) with (x, y) coordinates.\n",
    "    - k, t, d: RANSAC parameters.\n",
    "    - min_remaining_points: minimum number of points required to run RANSAC on remaining data.\n",
    "    \n",
    "    Returns:\n",
    "    - lines: list of tuples (model, inlier_points) for each detected line.\n",
    "    - remaining_data: points that were not fitted to any line.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    remaining_data = data.copy()\n",
    "\n",
    "    while remaining_data.shape[0] >= min_remaining_points:\n",
    "        model, inlier_indices = ransac_line_vectorized(remaining_data, k, t, d)\n",
    "        if model is None or inlier_indices is None or inlier_indices.size == 0:\n",
    "            break\n",
    "\n",
    "        m, c = model\n",
    "        rho, theta = convert_line_to_rho_theta(m, c)\n",
    "\n",
    "        lines.append((rho, theta))\n",
    "\n",
    "        # Remove the inliers from the remaining data\n",
    "        remaining_data = np.delete(remaining_data, inlier_indices, axis=0)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1685,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_lines(lines, angle_tol=15):\n",
    "    \"\"\"Group lines by their theta angles (in degrees) with a tolerance.\"\"\"\n",
    "    groups = []\n",
    "    deg_tol = np.deg2rad(angle_tol)\n",
    "    \n",
    "    for line in lines:\n",
    "        rho, theta = line\n",
    "        matched = False\n",
    "        \n",
    "        for group in groups:\n",
    "            t_diff = abs(theta - group['theta'])\n",
    "            if min(t_diff, np.pi - t_diff) <= deg_tol:\n",
    "                group['lines'].append(line)\n",
    "                matched = True\n",
    "                break\n",
    "        \n",
    "        if not matched:\n",
    "            groups.append({'theta': theta, 'lines': [line]})\n",
    "    \n",
    "    return groups\n",
    "\n",
    "def compute_max_distance(group):\n",
    "    \"\"\"Compute maximum distance between lines in a group.\"\"\"\n",
    "    rhos = [line[0] for line in group['lines']]\n",
    "    max_rho, min_rho = max(rhos), min(rhos)\n",
    "    return max_rho - min_rho\n",
    "\n",
    "def find_perpendicular_pairs(groups, angle_tol=15):\n",
    "    \"\"\"Find pairs of perpendicular line groups.\"\"\"\n",
    "    pairs = []\n",
    "    deg_tol = np.deg2rad(angle_tol)\n",
    "    \n",
    "    for i, g1 in enumerate(groups):\n",
    "        for j, g2 in enumerate(groups[i+1:], i+1):\n",
    "            angle_diff = abs(g1['theta'] - g2['theta'])\n",
    "            if abs(min(angle_diff, np.pi - angle_diff) - np.pi/2) <= deg_tol:\n",
    "                pairs.append((g1, g2))\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "def line_intersection(line1, line2):\n",
    "    \"\"\"Find intersection point of two lines.\"\"\"\n",
    "    rho1, theta1 = line1\n",
    "    rho2, theta2 = line2\n",
    "    \n",
    "    A = np.array([\n",
    "        [np.cos(theta1), np.sin(theta1)],\n",
    "        [np.cos(theta2), np.sin(theta2)]\n",
    "    ])\n",
    "    b = np.array([[rho1], [rho2]])\n",
    "    \n",
    "    try:\n",
    "        x, y = np.linalg.solve(A, b)\n",
    "        return int(np.round(x)), int(np.round(y))\n",
    "    except np.linalg.LinAlgError:\n",
    "        return None\n",
    "\n",
    "def find_largest_rectangle(lines):\n",
    "    if lines is None:\n",
    "        # print(\"No lines detected\")\n",
    "        return\n",
    "    \n",
    "    # Group lines and filter small groups\n",
    "    groups = group_lines(lines)\n",
    "    groups = [g for g in groups if len(g['lines']) >= 2]\n",
    "    \n",
    "    # Find perpendicular pairs and calculate areas\n",
    "    pairs = find_perpendicular_pairs(groups)\n",
    "    max_area = 0\n",
    "    best_pair = None\n",
    "    \n",
    "    for g1, g2 in pairs:\n",
    "        area = compute_max_distance(g1) * compute_max_distance(g2)\n",
    "        if area > max_area:\n",
    "            max_area = area\n",
    "            best_pair = (g1, g2)\n",
    "    \n",
    "    if not best_pair:\n",
    "        # print(\"No perpendicular pairs found\")\n",
    "        return\n",
    "    \n",
    "    # Get extreme lines from best pair\n",
    "    g1_lines = sorted(best_pair[0]['lines'], key=lambda x: x[0])\n",
    "    g2_lines = sorted(best_pair[1]['lines'], key=lambda x: x[0])\n",
    "    \n",
    "    l1_min = g1_lines[0]\n",
    "    l1_max = g1_lines[-1]\n",
    "    l2_min = g2_lines[0]\n",
    "    l2_max = g2_lines[-1]\n",
    "    \n",
    "    # Find intersection points\n",
    "    pts = [\n",
    "        line_intersection(l1_min, l2_min),\n",
    "        line_intersection(l1_min, l2_max),\n",
    "        line_intersection(l1_max, l2_max),\n",
    "        line_intersection(l1_max, l2_min)\n",
    "    ]\n",
    "    \n",
    "    if None in pts:\n",
    "        # print(\"Could not find all intersection points\")\n",
    "        return\n",
    "    \n",
    "    return pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1686,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_points(pts):\n",
    "    \"\"\"Sort points to [TL, TR, BR, BL] order.\"\"\"\n",
    "    pts = np.array(pts)\n",
    "    x_sorted = pts[np.argsort(pts[:, 0])]\n",
    "    left = x_sorted[:2]\n",
    "    right = x_sorted[2:]\n",
    "    left = left[np.argsort(left[:, 1])]\n",
    "    tl, bl = left[0], left[1]\n",
    "    right = right[np.argsort(right[:, 1])]\n",
    "    tr, br = right[0], right[1]\n",
    "    return np.array([tl, tr, br, bl], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1687,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_homography(src_points, dst_points):\n",
    "    # Compute homography matrix from source to destination points.\n",
    "    A = []\n",
    "    b = []\n",
    "    for i in range(4):\n",
    "        x, y = src_points[i]\n",
    "        u, v = dst_points[i]\n",
    "        A.append([x, y, 1, 0, 0, 0, -x*u, -y*u])\n",
    "        A.append([0, 0, 0, x, y, 1, -x*v, -y*v])\n",
    "        b.extend([u, v])\n",
    "    A = np.array(A, dtype=np.float64)\n",
    "    b = np.array(b, dtype=np.float64)\n",
    "    h = np.linalg.lstsq(A, b, rcond=None)[0]\n",
    "    H = np.array([\n",
    "        [h[0], h[1], h[2]],\n",
    "        [h[3], h[4], h[5]],\n",
    "        [h[6], h[7], 1]\n",
    "    ], dtype=np.float64)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1688,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, target_size):\n",
    "    # Resize image to target_size (width, height)\n",
    "    return cv.resize(image, target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1689,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warp_image_bilinear(image, H, output_shape):\n",
    "    output_height, output_width = output_shape\n",
    "    H_inv = np.linalg.inv(H)\n",
    "    \n",
    "    u, v = np.meshgrid(np.arange(output_width), np.arange(output_height))\n",
    "    grid = np.stack([u.ravel(), v.ravel(), np.ones_like(u.ravel())])\n",
    "    src_points = H_inv @ grid\n",
    "    \n",
    "    x = (src_points[0] / src_points[2]).reshape(output_height, output_width)\n",
    "    y = (src_points[1] / src_points[2]).reshape(output_height, output_width)\n",
    "    \n",
    "    # Floor and ceiling coordinates for bilinear interpolation\n",
    "    x0 = np.floor(x).astype(int)\n",
    "    y0 = np.floor(y).astype(int)\n",
    "    x1 = x0 + 1\n",
    "    y1 = y0 + 1\n",
    "    \n",
    "    # Clamp coordinates to prevent out-of-bounds\n",
    "    x0 = np.clip(x0, 0, image.shape[1]-1)\n",
    "    x1 = np.clip(x1, 0, image.shape[1]-1)\n",
    "    y0 = np.clip(y0, 0, image.shape[0]-1)\n",
    "    y1 = np.clip(y1, 0, image.shape[0]-1)\n",
    "    \n",
    "    # Interpolation weights\n",
    "    wa = (x1 - x) * (y1 - y)\n",
    "    wb = (x1 - x) * (y - y0)\n",
    "    wc = (x - x0) * (y1 - y)\n",
    "    wd = (x - x0) * (y - y0)\n",
    "    \n",
    "    # Sample pixel values\n",
    "    Ia = image[y0, x0]\n",
    "    Ib = image[y1, x0]\n",
    "    Ic = image[y0, x1]\n",
    "    Id = image[y1, x1]\n",
    "    \n",
    "    # Compute interpolated values\n",
    "    warped = (wa[..., np.newaxis] * Ia +\n",
    "              wb[..., np.newaxis] * Ib +\n",
    "              wc[..., np.newaxis] * Ic +\n",
    "              wd[..., np.newaxis] * Id).astype(np.uint8)\n",
    "    \n",
    "    return warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1690,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warp_image(path):\n",
    "    img = resize_image(cv.imread(path), (1024, 1024))\n",
    "\n",
    "    img_gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    img_blurred = cv.GaussianBlur(img_gray, (13, 13), 0)\n",
    "\n",
    "    edges = cv.Canny(img_blurred, 50, 150)\n",
    "\n",
    "    accumulator, thetas, rhos = hough_transform(edges)\n",
    "    lines = hough_lines(accumulator, thetas, rhos, threshold=100)\n",
    "\n",
    "    points = np.column_stack(np.where(edges > 0))[:, ::-1]\n",
    "\n",
    "    lines_ransac = find_multiple_lines(points, 1000, 2, 150)\n",
    "\n",
    "    lines.extend(lines_ransac)\n",
    "\n",
    "    pts = find_largest_rectangle(lines)\n",
    "\n",
    "    if not pts:\n",
    "        return img\n",
    "\n",
    "    ordered_corners = order_points(pts)\n",
    "\n",
    "    # Calculate destination size (width and height)\n",
    "    tl, tr, br, bl = ordered_corners\n",
    "    width = int(max(np.linalg.norm(tr - tl), np.linalg.norm(br - bl)))\n",
    "    height = int(max(np.linalg.norm(bl - tl), np.linalg.norm(br - tr)))\n",
    "\n",
    "    # Define destination points (rectangle)\n",
    "    dst_points = np.array([\n",
    "        [0, 0],\n",
    "        [width, 0],\n",
    "        [width, height],\n",
    "        [0, height]\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    # Compute homography matrix\n",
    "    H = compute_homography(ordered_corners, dst_points)\n",
    "\n",
    "    if height < 7 or width < 7:\n",
    "        return img\n",
    "\n",
    "    # Warp the image\n",
    "    warped_image = warp_image_bilinear(img, H, (height, width))\n",
    "\n",
    "    return warped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1691,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ssim(imageA, imageB):\n",
    "    # Convert images to grayscale for SSIM computation\n",
    "    grayA = cv.cvtColor(imageA, cv.COLOR_BGR2GRAY)\n",
    "    grayB = cv.cvtColor(imageB, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    height, width = grayB.shape\n",
    "\n",
    "    grayA_resized = resize_image(grayA, (width, height))\n",
    "\n",
    "    score, _ = ssim(grayA_resized, grayB, full=True)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1692,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]C:\\Users\\ahmtk\\AppData\\Local\\Temp\\ipykernel_27964\\3768751934.py:54: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return int(np.round(x)), int(np.round(y))\n",
      "100%|██████████| 100/100 [01:54<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM for folder (curved): 0.20276961879593677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:52<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM for folder (fold): 0.20094835407299386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:38<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM for folder (incomplete): 0.21946030064507657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:48<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM for folder (perspective): 0.2202182650132417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:16<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM for folder (random): 0.20564519723268673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:06<00:00,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM for folder (rotate): 0.2570046922940457\n",
      "Average SSIM over all pairs: 0.21767440467566354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_path = \"WarpDoc\"\n",
    "\n",
    "digital = os.path.join(start_path, \"digital\")\n",
    "distorted = os.path.join(start_path, \"distorted\")\n",
    "\n",
    "total_ssim_scores = []\n",
    "\n",
    "for folder in os.listdir(distorted):\n",
    "    digital_path = os.path.join(digital, folder)\n",
    "    distorted_path = os.path.join(distorted, folder)\n",
    "\n",
    "    ssim_scores = []\n",
    "\n",
    "    for image_path in tqdm(os.listdir(distorted_path)):\n",
    "        pathA = os.path.join(digital_path, image_path)\n",
    "        pathB = os.path.join(distorted_path, image_path)\n",
    "        \n",
    "        image = resize_image(cv.imread(pathA), (1024, 1024))\n",
    "        warped_image = warp_image(pathB)\n",
    "\n",
    "        if image is None or warped_image is None:\n",
    "            print(f\"Error loading image: {pathA} or {pathB}\")\n",
    "\n",
    "        score = compute_ssim(image, warped_image)\n",
    "        ssim_scores.append(score)\n",
    "\n",
    "    total_ssim_scores.extend(ssim_scores)\n",
    "    \n",
    "    average_ssim = sum(ssim_scores) / len(ssim_scores)\n",
    "    print(f\"SSIM for folder ({folder}): {average_ssim}\")\n",
    "\n",
    "# Calculate and print the average SSIM if there are valid image pairs\n",
    "if total_ssim_scores:\n",
    "    average_total_ssim = sum(total_ssim_scores) / len(total_ssim_scores)\n",
    "    print(f\"Average SSIM over all pairs: {average_total_ssim}\")\n",
    "else:\n",
    "    print(\"No valid image pairs to compute SSIM.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
